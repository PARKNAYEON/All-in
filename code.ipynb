{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "included-brisbane",
   "metadata": {},
   "source": [
    "# 웹스크래핑  \n",
    "네이버 종목 토론방에서 작성일자/제목(댓글 수도 제목에 포함)/조회수/추천수/비추천수를 수접함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver # selenium 제어창에 대한 옵션\n",
    "options=webdriver.ChromeOptions()\n",
    "options.add_argument('--privileged')\n",
    "options.add_argument('headless')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--start-maximized')\n",
    "options.add_argument('disable-gpu')  # GPU 사용 안함\n",
    "options.add_argument('lang=ko_KR')  # 언어 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 재설정\n",
    "try:\n",
    "    os.chdir(\"C:\\\\Users\\\\SAMSUNG\\\\Desktop\\\\\") # Chromewebdriver가 있는 경로로 설정 \n",
    "    print(\"Directory changed\")\n",
    "except OSError: # 예외처리\n",
    "    print(\"Can't change the Current Working Directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_data(directory): # 하위 디렉토리 생성\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(\"Directory created\")\n",
    "    except OSError:\n",
    "        print('Error: Creating directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraping(code, start_page,end_page, count): # 종목토론방 긁어오기\n",
    "    title_result=[]\n",
    "    date_result=[]\n",
    "    like_result=[]\n",
    "    dislike_result=[]\n",
    "    views_result=[]\n",
    "\n",
    "    driver=webdriver.Chrome(\"chromedriver.exe\", options=options)\n",
    "    url=\"https://finance.naver.com/item/board.nhn?code=\"+code+\"&page=\"+str(start_page)\n",
    "    driver.get(url)\n",
    "    while (start_page<=end_page):\n",
    "        if (start_page==1): # 첫 페이지\n",
    "            page=start_page\n",
    "        elif (start_page<=10): # 2~10 페이지, 밑의 페이지 bar 구성이 다름\n",
    "            page=start_page+1\n",
    "        else:\n",
    "            if (start_page%10==0): # view의 마지막 페이지인 경우\n",
    "                page=12\n",
    "            else:\n",
    "                page=start_page%10+2\n",
    "        html = driver.page_source\n",
    "        soup=bs(html,'lxml') # selenium의 각 화면에서 bs4를 이용하여 긁어옴\n",
    "        \n",
    "        # 각 요소를 끌어오는 부분\n",
    "        title_css_selector=\"#content > div.section.inner_sub > table.type2 > tbody > tr > td.title > a\"\n",
    "        title=soup.select(title_css_selector)\n",
    "        for i in title: \n",
    "            title_result.append(i.text.replace('\\t','').replace('\\n',''))\n",
    "    \n",
    "        date_css_selector=\"#content > div.section.inner_sub > table.type2 > tbody > tr > td:nth-of-type(1) > span\"\n",
    "        date=soup.select(date_css_selector)\n",
    "        for j in date:\n",
    "            date_result.append(j.text)\n",
    "    \n",
    "        views_css_selector=\"#content > div.section.inner_sub > table.type2 > tbody > tr > td:nth-of-type(4) > span\"\n",
    "        views=soup.select(views_css_selector)\n",
    "        for k in views:\n",
    "            views_result.append(k.text)\n",
    "    \n",
    "\n",
    "        like_css_selector=\"#content > div.section.inner_sub > table.type2 > tbody > tr > td:nth-of-type(5) > strong\"\n",
    "        like=soup.select(like_css_selector)\n",
    "        for l in like:\n",
    "            like_result.append(l.text)\n",
    "    \n",
    "        dislike_css_selector=\"#content > div.section.inner_sub > table.type2 > tbody > tr > td:nth-of-type(6) > strong\"\n",
    "        dislike=soup.select(dislike_css_selector)\n",
    "        for m in dislike:\n",
    "            dislike_result.append(m.text)\n",
    "        \n",
    "        # 자동으로 다음 새 10개 페이지로 이동하는 부분\n",
    "        nextpg_css_selector=f\"#content > div.section.inner_sub > table:nth-child(3) > tbody > tr > td:nth-child(2) > table > tbody > tr > td:nth-child({page+1}) > a\"\n",
    "        nextpg_element=driver.find_element_by_css_selector(nextpg_css_selector)\n",
    "        nextpg_element.click()\n",
    "        start_page+=1\n",
    "    # 모인 요소들을 합치는 부분\n",
    "    concating(code+'.KS', title_result,date_result,views_result,like_result,dislike_result,count)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concating(code, title_result,date_result,views_result,like_result,dislike_result,count): # 긁어온 데이터를 합치는 함수\n",
    "    concat_data={'title':title_result,\n",
    "                'date':date_result,\n",
    "                'views':views_result,\n",
    "                'like':like_result,\n",
    "                'dislike':dislike_result}\n",
    "    scraping_df=pd.DataFrame(concat_data)\n",
    "    scraping_df['Code']=code\n",
    "    scraping_df.to_csv('./title_scraping_'+code+'_'+str(count)+'.csv',mode='w',encoding='utf-8-sig',header=True,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    code=input(\"기업 코드를 입력해주세요.: \")\n",
    "    start_page=int(input(\"첫 번째 페이지를 입력해주세요.: \"))\n",
    "    end_page=int(input(\"마지막 페이지를 입력해주세요.: \"))\n",
    "    mkdir_data(code+'_data') # 디렉토리 생성\n",
    "    count=1\n",
    "    while (start_page<=end_page):\n",
    "        if (end_page-start_page<100): # (1)\n",
    "            web_scraping(code,start_page,end_page,count)\n",
    "        else:\n",
    "            web_scraping(code,start_page,start_page+100,count) # (2)\n",
    "        print(\"%d번째 스크래핑이 끝났습니다.\"%count)\n",
    "        start_page+=101 # (3), (1)~(3)의 숫자는 임의 조정 가능. \n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-madness",
   "metadata": {},
   "source": [
    "# 파일 모으기  \n",
    "여러개로 나누어진 종목토론방 데이터를 하나로 묶는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_file(input_path, merge_output):\n",
    "    file_list=glob.glob(os.path.join(input_path,'*'))\n",
    "    with open(merge_output,'w',encoding='utf-8-sig') as f:\n",
    "        for i, file in enumerate(file_list):\n",
    "            if i==0: # 처음 파일에만 헤더 포함!\n",
    "                with open(file,'r',encoding='utf-8-sig') as f2:\n",
    "                    while True:\n",
    "                        line=f2.readline()\n",
    "                        \n",
    "                        if not line:\n",
    "                            break\n",
    "                        f.write(line)\n",
    "                file_name=file.split('\\\\')[-1]\n",
    "                print(file_name+ ' complete')\n",
    "            else:\n",
    "                with open(file,'r',encoding='utf-8-sig') as f2:\n",
    "                    n=0\n",
    "                    while True:\n",
    "                        line=f2.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        if n!=0:\n",
    "                            f.write(line)\n",
    "                        n+=1\n",
    "                file_name=file.split('\\\\')[-1]\n",
    "                print(file_name+ ' complete')\n",
    "    print(\"모든 파일 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path=input(\"경로를 입력해주세요. :\") # 기업별 하위 디렉토리 경로 ./기업코드_data 형식\n",
    "input_path.replace(\"\\\\\",\"\\/\") # 복사 붙여넣기 해도 \\를 /로 바꿔준다.\n",
    "input_path=input_path+'\\/' # 맨 마지막에도 채워 넣어 준다.\n",
    "output_name=input(\"최종 파일 이름을 입력해주세요. :\")\n",
    "output_path=input(\"최종 파일 경로를 입력해주세요. :\")\n",
    "output_path.replace(\"\\\\\",\"\\/\")\n",
    "merge_output=output_path+'\\\\'+output_name+'.csv'\n",
    "merge_file(input_path,merge_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-thesaurus",
   "metadata": {},
   "source": [
    "# 형태소 분석  \n",
    "모인 데이터를 바탕으로 형태소를 분석한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rhinoMorph # 형태소 분석기\n",
    "import re\n",
    "rn = rhinoMorph.startRhino()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Morph(df):\n",
    "    print(df.isna().sum()) # 결측치 확인\n",
    "    comment(df)\n",
    "    \n",
    "    morphed_data=''\n",
    "\n",
    "    for i in df['adj_title']:\n",
    "        morphed_data_object=rhinoMorph.onlyMorph_list(rn,i,pos=['NNG', 'VCP', 'VCN', 'MAG', 'VA', 'VV', 'XR','MM','NV','NF'],eomi=True)\n",
    "        joined_data_object=' '.join(morphed_data_object)\n",
    "    \n",
    "    if joined_data_object:\n",
    "        morphed_data+=joined_data_object+'\\n'\n",
    "        \n",
    "    merge_text_list=re.split('\\n| ',morphed_data)\n",
    "    word=pd.Series(merge_text_list)\n",
    "    result=word.value_counts()\n",
    "    \n",
    "    result.to_csv('result.csv',mode='w',encoding='utf-8-sig',header=True,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(df): # 제목에서 댓글 수 뽑아내기. 댓글은 [n]의 형태\n",
    "    df['comment']=0\n",
    "    df['adj_title']=''\n",
    "    \n",
    "    k=0\n",
    "    for j in df['title']:\n",
    "        if j:\n",
    "            if j[-1]==']': \n",
    "                df['comment'][k]=float(j[-2])\n",
    "                df['adj_title'][k]=j[:-3]\n",
    "            else:\n",
    "                df['adj_title'][k]=j\n",
    "        k+=1\n",
    "        \n",
    "    df['date']=pd.to_datetime(df['date'],format='%Y.%m.%d %H:%M') # 문자열로 받은 날짜 데이터 포맷 변경\n",
    "    df.drop(labels=['Unnamed: 0','title'],axis=1,inplace=True) # 원래 타이틀은 댓글 수를 포함하고 있으므로 분리 이후에 삭제\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('./merge_output.csv', encoding='utf-8-sig') # 파일 이름은 merge_file 함수를 거치고 난 결과물을 대입\n",
    "Morph(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-hamburg",
   "metadata": {},
   "source": [
    "# Levenshtein Algorithm  \n",
    "형태소 분리 된 데이터에 대해 편집거리 알고리즘을 적용한다.  \n",
    "levenshtein을 두 개로 분리한 이유? = 파이썬의 for문 호출 제한 이슈 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jamo import h2j, j2hcj # 자모 분리 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_cost(a,b): # 교체비용에 해당하는 함수\n",
    "    if a==b: # 두 문자가 아예 같을 경우\n",
    "        return 0\n",
    "    return round(levenshtein(j2hcj(h2j(a)),j2hcj(h2j(b)))/3,2) # 다르다면 자모 단위까지 levenshtein을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jamo_levenshtein(a, b): # 자모단위 levenshtein\n",
    "    al=len(a)\n",
    "    bl=len(b)\n",
    "    if a==b: #문자열이 같을 때\n",
    "        return 0\n",
    "    if al<bl:\n",
    "        return jamo_levenshtein(b,a)\n",
    "    if b=='': #두 번째 문자열이 공백일 경우\n",
    "        return al\n",
    "    \n",
    "    matrix=np.zeros((al+1,bl+1)) #모든값이 0인 행렬 반환\n",
    "    \n",
    "    for i in range(al+1): #초기화\n",
    "        matrix[i][0]=i\n",
    "    for j in range(bl+1):\n",
    "        matrix[0][j]=j\n",
    "    \n",
    "    for i in range(1, al+1):\n",
    "        aw=a[i-1]\n",
    "        for j in range(1, bl+1):\n",
    "            bw=b[j-1]\n",
    "            matrix[i][j]=min({\n",
    "                matrix[i-1][j]+1, #삭제\n",
    "                matrix[i][j-1]+1, #삽입\n",
    "                matrix[i-1][j-1]+(aw!=bw) #다를경우 1추가\n",
    "            })\n",
    "    return matrix[-1][-1] #행렬의 오른쪽 최하단값 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(a, b): # 글자단위 levenshtein\n",
    "    al=len(a)\n",
    "    bl=len(b)\n",
    "    if a==b: #문자열이 같을 때\n",
    "        return 0\n",
    "    if al<bl:\n",
    "        return levenshtein(b,a)\n",
    "    if b=='': #두 번째 문자열이 공백일 경우\n",
    "        return al\n",
    "    \n",
    "    matrix=np.zeros((al+1,bl+1)) #모든값이 0인 행렬 반환\n",
    "    \n",
    "    for i in range(al+1): #초기화\n",
    "        matrix[i][0]=i\n",
    "    for j in range(bl+1):\n",
    "        matrix[0][j]=j\n",
    "    \n",
    "    for i in range(1, al+1):\n",
    "        aw=a[i-1]\n",
    "        for j in range(1, bl+1):\n",
    "            bw=b[j-1]\n",
    "            matrix[i][j]=min({\n",
    "                matrix[i-1][j]+1, #삭제\n",
    "                matrix[i][j-1]+1, #삽입\n",
    "                matrix[i-1][j-1]+change_cost(aw,bw) #다를경우 1추가\n",
    "            })\n",
    "    return matrix[-1][-1] #행렬의 오른쪽 최하단값 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:\\\\Users\\\\SAMSUNG\\\\Downloads\\\\전체형태소_삼성전자.csv')\n",
    "data.rename(columns={'Unnamed: 0':'형태소', '0':'빈도수','Unnamed: 2':'감성점수','Unnamed: 3':'비고'},inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_low_frequency=data[(data.빈도수<20)&(data.빈도수>17)] # 숫자는 상황에 따라 임의 조정\n",
    "data_with_sent=data[data.빈도수>=20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={}\n",
    "result=[]\n",
    "\n",
    "for base in data_low_frequency['형태소']:\n",
    "    for source in data_with_sent['형태소']:\n",
    "        if levenshtein(base,source)<0.5:\n",
    "            word_dict[source]=levenshtein(base,source)\n",
    "    print(base)\n",
    "    print(word_dict)\n",
    "    \n",
    "    if any(word_dict): # 만약 가까운 요소가 출력이 안된다면 base를 그대로 사용\n",
    "        result.append(base)\n",
    "        continue\n",
    "\n",
    "    ans=input(\"출력된 단어 중 가장 가까운 단어를 골라 써주세요(없으면 x입력): \")\n",
    "    if ans in word_dict:\n",
    "        result.append(ans)\n",
    "        print(\"단어가 대체되어 등록되었습니다.\")\n",
    "    else if ans=='x':\n",
    "        print(\"선택하지 않으셨습니다.\")\n",
    "        result.append(base) # 출력된 단어 중 어느것과도 가깝지 않으면 base를 그대로 사용\n",
    "    else:\n",
    "        print(\"찾을 수 없는 단어입니다.\")\n",
    "        \n",
    "    word_dict={} #초기화\n",
    "    \n",
    "data_low_frequency['형태소']=result # 바뀐 값 복사"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-aggregate",
   "metadata": {},
   "source": [
    "# 주가 데이터 불러오기(일단위)  \n",
    "pandas-datareader사용, 최근 5년의 주가데이터를 불러온다.  \n",
    "pandas-datareader를 사용하는 이유? = 야후 사이트만이 수정종가를 제공하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "code={} # 코드 딕셔너리를 불러와 Datareader 인수로 전달한다.\n",
    "code=code_dict(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stock_code, stock_name in code.items():\n",
    "    stock_data=web.DataReader(stock_code,'yahoo')\n",
    "    stock_data['기업명']=stock_name\n",
    "    stock_data.to_csv(stock_name+'_alltime_주가데이터.csv',encoding='utf-8-sig')\n",
    "\n",
    "print('작업완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-hollow",
   "metadata": {},
   "source": [
    "# 주가 데이터 불러오기(실시간)  \n",
    "네이버 금융에서 실시간으로 변하는 주가를 불러오는 부분.  \n",
    "종목 토론방의 데이터를 긁어오는 매커니즘과 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_scraping(code, stock_date,stock_time):\n",
    "    time_result=[]\n",
    "    price_result=[]\n",
    "    pivot=1\n",
    "    \n",
    "    driver=webdriver.Chrome(\"chromedriver.exe\", options=options)\n",
    "    url='https://finance.naver.com/item/sise_time.nhn?code='+code+'&thistime='+str(stock_date)+str('stock_time')\n",
    "    driver.get(url)\n",
    "    \n",
    "    while True:\n",
    "        if pivot==1:\n",
    "            page=pivot\n",
    "        elif(pivot<=10):\n",
    "            page=pivot+1\n",
    "        else:\n",
    "            if(pivot%10==0):\n",
    "                page=12\n",
    "            else:\n",
    "                page=(pivot%10)+2\n",
    "            \n",
    "        html = driver.page_source\n",
    "        soup=bs(html,'lxml')\n",
    "\n",
    "        time_css_selector=\"body > table.type2 > tbody > tr > td:nth-of-type(1) > span\"\n",
    "        now_time=soup.select(time_css_selector)\n",
    "        for i in now_time:\n",
    "            time_result.append(i.text)\n",
    "    \n",
    "        price_css_selector=\"body > table.type2 > tbody > tr > td:nth-of-type(2) > span\"\n",
    "        price=soup.select(price_css_selector)\n",
    "        for j in price:\n",
    "            price_result.append(j.text)\n",
    "        \n",
    "        nextpg_css_selector=f\"body > table.Nnavi > tbody > tr > td:nth-child({page+1}) > a\"    \n",
    "        try:\n",
    "            nextpg_element=driver.find_element_by_css_selector(nextpg_css_selector)\n",
    "            nextpg_element.click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"마지막 페이지\")\n",
    "            break\n",
    "        pivot+=1\n",
    "\n",
    "    concating(code, time_result,price_result,stock_time)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concating(code, time_result, price_result, stock_time):\n",
    "    date_result=stock_date\n",
    "    concat_data={'date':date_result,\n",
    "                 'time':time_result,\n",
    "                'price':price_result}\n",
    "    scraping_df=pd.DataFrame(concat_data)\n",
    "    scraping_df.to_csv('./stock_crwaling_'+code+'_'+str(date_result)+'.csv',mode='w',encoding='utf-8-sig',header=True,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "code=input(\"기업 코드를 입력해주세요.: \")\n",
    "stock_date=int(input(\"날짜를 입력해주세요.(예시: 20210207): \"))\n",
    "stock_time=int(input(\"시간을 입력해주세요.(예시: 161059): \"))\n",
    "stock_scraping(code,stock_date,stock_time)\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-civilization",
   "metadata": {},
   "source": [
    "# 기업코드 매칭  \n",
    "(기업코드6자리).KS 형태로 저장되어 있는 데이터들을 기업 코드에 맞는 기업 명으로 매칭하는 작업  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_dict(dictionary): # 코드-기업명 딕셔너리 생성.\n",
    "    data=open('기업코드.txt','r',encoding='UTF8')\n",
    "    line=data.readlines()\n",
    "    data.close()\n",
    "    code_key=[]\n",
    "    code_value=[]\n",
    "    for i in line:\n",
    "        temp=i[:-1] # 줄바꿈 제거\n",
    "        code_key.append(temp.split(\" \")[1]+'.KS') # 공백기준 코드만 뽑기\n",
    "        code_value.append(temp.split(\" \")[0]) # 공백기준 기업명 뽑기\n",
    "    code=dict(zip(code_key,code_value)) # zip은 인덱스 기준으로 리스트를 순서대로 가져오는 함수, dict는 딕셔너리 변환\n",
    "    return code    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_matching(df,code):\n",
    "    code_key_list=list(code.keys())\n",
    "    code_name_list=list(code.values())\n",
    "    match_code=[]\n",
    "    for i, data in enumerate(df['Code']):\n",
    "        for j, obj in enumerate(code_key_list):\n",
    "            if data==obj:\n",
    "                match_code.append(code_name_list[j])\n",
    "    df['기업명']=match_code"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
